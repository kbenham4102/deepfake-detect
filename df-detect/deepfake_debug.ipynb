{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd \n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, Conv2D, Flatten, Dense, BatchNormalization\n",
    "import tensorflow.math as M\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from video_loader import DeepFakeDualTransformer, DeepFakeTransformer\n",
    "import sys\n",
    "import pathlib\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimilarityLoss(tf.keras.losses.Loss):\n",
    "    # Loss function subclassed to penalize for returning a pair \n",
    "    # with a high cosine sim\n",
    "    def call(self, y_real, y_fake):\n",
    "        cosine_sim = M.reduce_sum(M.multiply(y_real, y_fake))/(M.reduce_euclidean_norm(y_real) * M.reduce_euclidean_norm(y_fake))\n",
    "        loss = M.scalar_mul(-1, M.log(1 - cosine_sim))\n",
    "\n",
    "        return loss\n",
    "@tf.function\n",
    "def similarity_loss(y_real, y_fake):\n",
    "    \n",
    "    cosine_sim = M.reduce_sum(M.multiply(y_real, y_fake))/(M.reduce_euclidean_norm(y_real) * M.reduce_euclidean_norm(y_fake))\n",
    "    loss = M.scalar_mul(-1, M.log(1 - cosine_sim))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def load_from_file_pair(real_path, fake_path, transformer):\n",
    "  \n",
    "    real_vid = transformer.transform_vid(real_path)[0]\n",
    "    fake_vid = transformer.transform_vid(fake_path)[0]\n",
    "\n",
    "    return tf.stack((real_vid, fake_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some params\n",
    "# model params\n",
    "load_ckpt = False # set false if not loading\n",
    "\n",
    "\n",
    "# Train params\n",
    "EPOCHS=10\n",
    "batch_size=1\n",
    "reg_penalty = 0.001\n",
    "cls_wt = {0:3, 1:2.25}\n",
    "validate_epochs = list(np.arange(EPOCHS))\n",
    "\n",
    "\n",
    "# Dataset params\n",
    "data_pairs_path = '../data/source/labels/fake_to_real_mapping.csv'\n",
    "resize_shape = (224,224)\n",
    "sequence_len = 30\n",
    "prefetch_num = 4\n",
    "train_val_split = 0.015\n",
    "\n",
    "\n",
    "# Final model params\n",
    "dt = datetime.datetime.now()\n",
    "tstamp = f'{dt.year}{dt.month}{dt.day}{dt.hour}:{dt.minute}:{dt.second}'\n",
    "regstr = str(reg_penalty).split('.')[1]\n",
    "\n",
    "model_stamp = tstamp + f'_model_{regstr}_reg'\n",
    "final_model_path = f'models/{model_stamp}/model.h5'\n",
    "checkpoint_prefix = f'models/{model_stamp}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND CPU AT:  /physical_device:CPU:0\n",
      "['/physical_device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Losses, metrics, optimizer\n",
    "\n",
    "# loss = SimilarityLoss\n",
    "\n",
    "# Define dummy test dims based on parameters\n",
    "test_dims = (None, None, *resize_shape, 3)\n",
    "\n",
    "# Get device names\n",
    "\n",
    "cpu = tf.config.experimental.list_physical_devices('CPU')[0].name\n",
    "print(\"FOUND CPU AT: \", cpu)\n",
    "print([x[0] for x in tf.config.experimental.list_physical_devices('GPU')])\n",
    "\n",
    "\n",
    "# Define and load the datasets\n",
    "df_pairs = pd.read_csv(data_pairs_path)[['real', 'fake']]\n",
    "train_df, val_df = train_test_split(df_pairs, test_size = train_val_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset from pairs of path strings\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices(train_df.to_numpy())\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices(val_df.to_numpy())\n",
    "\n",
    "# # TODO add in random crops, rotations, etc to make this non-redundant\n",
    "# # define the transformer to load data on the fly\n",
    "# train_transformer = DeepFakeDualTransformer(resize_shape=resize_shape, seq_length=sequence_len)\n",
    "# val_transformer = DeepFakeDualTransformer(resize_shape=resize_shape, seq_length=sequence_len)\n",
    "\n",
    "# # map the transformer on the dataset entries\n",
    "# train_ds = train_ds.map(lambda x: train_transformer.transform_map(x)).prefetch(prefetch_num)\n",
    "# val_ds = val_ds.map(lambda x: val_transformer.transform_map(x)).prefetch(prefetch_num)\n",
    "\n",
    "\n",
    "X_train_len = len(list(train_df))\n",
    "X_val_len = len(list(val_df))\n",
    "num_train_batches = int(np.ceil(X_train_len/batch_size))\n",
    "num_val_batches = int(np.ceil(X_val_len/batch_size))\n",
    "\n",
    "transformer = DeepFakeTransformer(resize_shape=resize_shape, seq_length=sequence_len) \n",
    "\n",
    "lr_fn = tf.keras.optimizers.schedules.PolynomialDecay(0.001, num_train_batches*EPOCHS, \n",
    "                                                      end_learning_rate=1e-5)\n",
    "optimizer = tf.keras.optimizers.SGD(lr_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, None, 111, 111, 64 154624    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 55, 55, 128) 885248    \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, None, 27, 27, 128) 1180160   \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 13, 13, 128)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 6, 6, 64)          73792     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "=================================================================\n",
      "Total params: 3,543,808\n",
      "Trainable params: 3,543,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model, currently copying model archs to model.py manually\n",
    "reg = tf.keras.regularizers.l2(l=reg_penalty)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(ConvLSTM2D(64, (3,3), strides=(2,2), \n",
    "                            padding='valid', \n",
    "                            data_format='channels_last',\n",
    "                            recurrent_regularizer=reg,\n",
    "                            kernel_regularizer=reg,\n",
    "                            bias_regularizer=reg, \n",
    "                            return_sequences=True,\n",
    "                            input_shape=test_dims[1:]))\n",
    "model.add(ConvLSTM2D(128, (3,3), strides=(2,2), \n",
    "                            padding='valid', \n",
    "                            data_format='channels_last',\n",
    "                            recurrent_regularizer=reg,\n",
    "                            kernel_regularizer=reg,\n",
    "                            bias_regularizer=reg,  \n",
    "                            return_sequences=True,\n",
    "                            ))\n",
    "model.add(ConvLSTM2D(128, (3,3), strides=(2,2), \n",
    "                            padding='valid', \n",
    "                            data_format='channels_last',\n",
    "                            recurrent_regularizer=reg,\n",
    "                            kernel_regularizer=reg,\n",
    "                            bias_regularizer=reg, \n",
    "                            return_sequences=True,\n",
    "                            ))\n",
    "model.add(ConvLSTM2D(128, (3,3), strides=(2,2), \n",
    "                            padding='valid', \n",
    "                            data_format='channels_last',\n",
    "                            recurrent_regularizer=reg,\n",
    "                            kernel_regularizer=reg,\n",
    "                            bias_regularizer=reg,  \n",
    "                            return_sequences=False,\n",
    "                            ))\n",
    "model.add(Conv2D(64 , (3,3), strides=(2,2), \n",
    "                        padding='valid', data_format='channels_last',\n",
    "                        kernel_regularizer=reg,\n",
    "                        bias_regularizer=reg,\n",
    "                        activation='relu'))\n",
    "model.add(Conv2D(64 , (3,3), strides=(2,2), \n",
    "                        padding='valid', data_format='channels_last',\n",
    "                        kernel_regularizer=reg,\n",
    "                        bias_regularizer=reg,\n",
    "                        activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='softmax'))\n",
    "model.compile()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_prefix, max_to_keep=5)\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "validation_loss_results = []\n",
    "\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def grad(model, x, loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pair = model(x)\n",
    "        loss_value = loss(y_pair[0], y_pair[1])\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    return loss_value, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 0 loss: 15.249\n",
      "Training batch 1 loss: 13.170\n",
      "Training batch 2 loss: 14.844\n",
      "Training batch 3 loss: 15.942\n",
      "Training batch 4 loss: 14.844\n",
      "Training batch 5 loss: 12.785\n",
      "Training batch 6 loss: 13.417\n",
      "Training batch 7 loss: 15.942\n",
      "Training batch 8 loss: 15.942\n",
      "Training batch 9 loss: 14.333\n",
      "Training batch 10 loss: 12.610\n",
      "Training batch 11 loss: 15.942\n",
      "Training batch 12 loss: 14.844\n",
      "Training batch 13 loss: 14.333\n",
      "Training batch 14 loss: 12.922\n",
      "Training batch 15 loss: 13.996\n",
      "Training batch 16 loss: 14.844\n",
      "Training batch 17 loss: 13.996\n",
      "Training batch 18 loss: 15.249\n",
      "Training batch 19 loss: 11.891\n",
      "Training batch 20 loss: 14.556\n",
      "Training batch 21 loss: 15.249\n",
      "Training batch 22 loss: 14.556\n",
      "Training batch 23 loss: 13.109\n",
      "Training batch 24 loss: 15.942\n",
      "Training batch 25 loss: 14.556\n",
      "Training batch 26 loss: 14.556\n",
      "Training batch 27 loss: inf\n",
      "Training batch 28 loss: nan\n",
      "Training batch 29 loss: nan\n",
      "Training batch 30 loss: nan\n",
      "Training batch 31 loss: nan\n",
      "Training batch 32 loss: nan\n",
      "Training batch 33 loss: nan\n",
      "Training batch 34 loss: nan\n",
      "Training batch 35 loss: nan\n",
      "Training batch 36 loss: nan\n",
      "Training batch 37 loss: nan\n",
      "Training batch 38 loss: nan\n",
      "Training batch 39 loss: nan\n",
      "Training batch 40 loss: nan\n",
      "Training batch 41 loss: nan\n",
      "Training batch 42 loss: nan\n",
      "Training batch 43 loss: nan\n",
      "Training batch 44 loss: nan\n",
      "Training batch 45 loss: nan\n",
      "Training batch 46 loss: nan\n",
      "Training batch 47 loss: nan\n",
      "Training batch 48 loss: nan\n",
      "Training batch 49 loss: nan\n",
      "Training batch 50 loss: nan\n",
      "Training batch 51 loss: nan\n",
      "Training batch 52 loss: nan\n",
      "Training batch 53 loss: nan\n",
      "Training batch 54 loss: nan\n",
      "Training batch 55 loss: nan\n",
      "Training batch 56 loss: nan\n",
      "Training batch 57 loss: nan\n",
      "Training batch 58 loss: nan\n",
      "Training batch 59 loss: nan\n",
      "Training batch 60 loss: nan\n",
      "Training batch 61 loss: nan\n",
      "Training batch 62 loss: nan\n",
      "Training batch 63 loss: nan\n",
      "Training batch 64 loss: nan\n",
      "Training batch 65 loss: nan\n",
      "Training batch 66 loss: nan\n",
      "Training batch 67 loss: nan\n",
      "Training batch 68 loss: nan\n",
      "Training batch 69 loss: nan\n",
      "Training batch 70 loss: nan\n",
      "Training batch 71 loss: nan\n",
      "Training batch 72 loss: nan\n",
      "Training batch 73 loss: nan\n",
      "Training batch 74 loss: nan\n",
      "Training batch 75 loss: nan\n",
      "Training batch 76 loss: nan\n",
      "Training batch 77 loss: nan\n",
      "Training batch 78 loss: nan\n",
      "Training batch 79 loss: nan\n",
      "Exception while reading movie ../data/source/train_val_sort/train/FAKE/qnwsmqpuao.mp4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f517c5e43215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mvids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_file_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cbeb48aef5e5>\u001b[0m in \u001b[0;36mload_from_file_pair\u001b[0;34m(real_path, fake_path, transformer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mreal_vid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_vid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfake_vid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_vid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_vid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepfake-proj/df-detect/video_loader.py\u001b[0m in \u001b[0;36mtransform_vid\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mout_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{filename} has incorrect video dimensions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# assert np.sum(np.isnan(vid)) == 0, f'{filename} has null pixels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepfake-proj/df-detect/video_loader.py\u001b[0m in \u001b[0;36mget_frames\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mall_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepfake-proj/df-detect/video_loader.py\u001b[0m in \u001b[0;36mread_frames\u001b[0;34m(self, path, num_frames, jitter, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mframe_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idxs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjitter_offsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_frames_at_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "        i = 0\n",
    "        # Training loop\n",
    "        for rp, fp in train_df.to_numpy():\n",
    "            \n",
    "            vids = load_from_file_pair(rp, fp, transformer)\n",
    "\n",
    "            # Optimize the model\n",
    "            loss_value, grads = grad(model, vids, similarity_loss)\n",
    "            \n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            print(\"Training batch {} loss: {:.3f}\".format(i, loss_value))\n",
    "            # Track progress\n",
    "            epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "            i += 1\n",
    "\n",
    "        # End epoch ops\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        manager.save(checkpoint_number=epoch)\n",
    "\n",
    "        \n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(np.random.randn(2,30,224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
