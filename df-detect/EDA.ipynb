{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "validate_epochs = [1,2,10]\n",
    "batch_size=1\n",
    "test_fraction = 0.2\n",
    "train_label_path = '../data/source/labels/train_meta.json'\n",
    "train_path = '../data/source/train/'\n",
    "checkpoint_prefix = 'models/ckpt_{epoch}'\n",
    "resize_shape = (224,224)\n",
    "sequence_len = 16\n",
    "n_workers = 1\n",
    "use_mult_prc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_process_train_targets(train_label_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_move_meta_json(path, label_path):\n",
    "    \n",
    "    json_file = glob.glob(path + '*.json')[0]\n",
    "    \n",
    "    cmd = 'mv ' + json_file + ' ' + label_path\n",
    "    print('Executing ', cmd)\n",
    "    os.system(cmd)\n",
    "    fname = json_file.split('/')[-1]\n",
    "    new_path = os.path.join(label_path, fname)\n",
    "    return new_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  mv ../data/source/train/train_meta.json ../data/source/labels/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/source/labels/train_meta.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_move_meta_json(train_path, '../data/source/labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = '../data/source/labels/train_meta.json'\n",
    "def sort_deepfake_train_examples(train_path, sorted_class_path, meta_path):\n",
    "    # Load in a dataframe to parse out move protocol for train examples\n",
    "    \n",
    "    df = load_process_train_targets(meta_path, train_path)\n",
    "    \n",
    "    fakes_path = os.path.join(sorted_class_path, 'FAKE/')\n",
    "    real_path = os.path.join(sorted_class_path, 'REAL/')\n",
    "    \n",
    "    # Check to see if path exists, if not, mkdir\n",
    "    if not os.path.exists(sorted_class_path):\n",
    "        os.mkdir(sorted_class_path)\n",
    "    if not os.path.exists(fakes_path):\n",
    "        os.mkdir(fakes_path)\n",
    "        print('created path ', fakes_path)\n",
    "    if not os.path.exists(real_path):\n",
    "        os.mkdir(real_path)\n",
    "        print('created path ', real_path)\n",
    "\n",
    "    n = len(df)\n",
    "    print(\"Num train videos \", n)\n",
    "    for i in range(n):\n",
    "        fpath = df.iloc[i].filepath\n",
    "        cls = df.iloc[i].target_class\n",
    "        \n",
    "        if cls == 1.0:\n",
    "            # if these are 1.0 they are a fake\n",
    "            os.system('mv ' + fpath + ' ' + fakes_path)\n",
    "            \n",
    "        else:\n",
    "            os.system('mv ' + fpath + ' ' + real_path)\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(\"Successfully sorted {} examples\".format(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train videos  400\n",
      "Successfully sorted 99 examples\n",
      "Successfully sorted 199 examples\n",
      "Successfully sorted 299 examples\n",
      "Successfully sorted 399 examples\n"
     ]
    }
   ],
   "source": [
    "sort_deepfake_train_examples(train_path, '../data/source/train_sorted/', meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_root = '/home/kevin/deepfake-proj/data/source/train_sorted'\n",
    "vid_root = pathlib.Path(vid_root)\n",
    "vid_ds = tf.data.Dataset.list_files(str(vid_root/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/kevin/deepfake-proj/data/source/train_sorted/FAKE/acqfdwsrhi.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_sorted/REAL/edyncaijwx.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_sorted/FAKE/aelzhcnwgf.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_sorted/FAKE/esxrvsgpvb.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_sorted/FAKE/bndybcqhfr.mp4'\n"
     ]
    }
   ],
   "source": [
    "for f in vid_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeTransformer(object):\n",
    "    def __init__(self, chan_means=[0.485, 0.456, 0.406],\n",
    "                       chan_std_dev=[0.229, 0.224, 0.225],\n",
    "                       resize_shape=(300,300),\n",
    "                       seq_length=298):\n",
    "        self.chan_means = chan_means\n",
    "        self.chan_std_dev = chan_std_dev\n",
    "        self.resize_shape = resize_shape\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def get_frames(filename):\n",
    "        '''\n",
    "        method for getting the frames from a video file\n",
    "        args: \n",
    "            filename: exact path of the video file\n",
    "            first_only: whether to detect the first frame only or all of the frames\n",
    "        out:\n",
    "            video_frames, label:  \n",
    "        '''\n",
    "\n",
    "        filepath = filename.numpy().decode('utf-8')\n",
    "\n",
    "\n",
    "        cap = cv2.VideoCapture(filepath) \n",
    "        # captures the video. Think of it as if life is a movie so we ask the method to focus on patricular event\n",
    "        # that is our video in this case. It will concentrate on the video\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        all_frames = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "\n",
    "\n",
    "        fc = 0\n",
    "        while(cap.isOpened() and fc < frameCount): # as long as all the frames have been traversed\n",
    "            ret, frame = cap.read()\n",
    "            # capture the frame. Again, if life is a movie, this function acts as camera\n",
    "\n",
    "            if ret==True:\n",
    "                all_frames[fc] = frame\n",
    "                fc += 1\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'): # break in between by pressing the key given\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        # release whatever was held by the method for say, resources and the video itself\n",
    "        return all_frames\n",
    "\n",
    "    # tensorflow functions to pre-process videos\n",
    "    def normalize(video, chan_means, chan_std_dev):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Arguments:\n",
    "            video {tf.Tensor} -- tensorflow reshaped video data\n",
    "            chan_means {array} -- [description]\n",
    "            chan_std_dev {array} -- [description]\n",
    "\n",
    "        Returns:\n",
    "            [tf.Tensor] -- normalized video data\n",
    "        \"\"\"\n",
    "\n",
    "        video /= 255\n",
    "        video -= chan_means\n",
    "        video /= chan_std_dev\n",
    "\n",
    "        return video\n",
    "\n",
    "    def transform_vid(self, filename):\n",
    "        \n",
    "        chan_means = self.chan_means\n",
    "        chan_std_dev = self.chan_std_dev\n",
    "        resize_shape = self.resize_shape\n",
    "        seq_length = self.seq_length\n",
    " \n",
    "        parts = tf.strings.split(filename, '/')\n",
    "        label = parts[-2]\n",
    "        # Don't want to exceed frames, available, using 198 as limit\n",
    "        if seq_length == 298:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = np.random.randint(298 - seq_length)\n",
    "\n",
    "        vid = get_frames(filename)[start:(start+seq_length),:,:,:]\n",
    "        vid = tf.image.resize(vid, size=resize_shape).numpy()\n",
    "        vid = normalize(vid, chan_means, chan_std_dev)\n",
    "\n",
    "        return vid, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DeepFakeTransformer(resize_shape=(224,224))\n",
    "trf_func = transformer.transform_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vid_ds = vid_ds.map(lambda x: tf.py_function(trf_func, [x], [tf.float32, tf.string]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'REAL', shape=(), dtype=string)\n",
      "(298, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "for vid, label in vid_ds.take(1):\n",
    "    print(label)\n",
    "    print(vid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224, 224]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.resize_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
