{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "validate_epochs = [1,2,10]\n",
    "batch_size=1\n",
    "test_fraction = 0.2\n",
    "train_label_path = '../data/source/labels/train_meta.json'\n",
    "train_path = '../data/source/train/'\n",
    "checkpoint_prefix = 'models/ckpt_{epoch}'\n",
    "resize_shape = (224,224)\n",
    "sequence_len = 16\n",
    "n_workers = 1\n",
    "use_mult_prc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_root = '/home/kevin/deepfake-proj/data/source/train_val_sort/train/'\n",
    "vid_root = pathlib.Path(vid_root)\n",
    "vid_ds = tf.data.Dataset.list_files(str(vid_root/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/kevin/deepfake-proj/data/source/train_val_sort/train/FAKE/eukvucdetx.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_val_sort/train/FAKE/ahfazfbntc.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_val_sort/train/FAKE/awhmfnnjih.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_val_sort/train/FAKE/dnhvalzvrt.mp4'\n",
      "b'/home/kevin/deepfake-proj/data/source/train_val_sort/train/REAL/ellavthztb.mp4'\n"
     ]
    }
   ],
   "source": [
    "for f in vid_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeTransformer(object):\n",
    "    def __init__(self, chan_means=[0.485, 0.456, 0.406],\n",
    "                       chan_std_dev=[0.229, 0.224, 0.225],\n",
    "                       resize_shape=(300,300),\n",
    "                       seq_length=298):\n",
    "        self.chan_means = chan_means\n",
    "        self.chan_std_dev = chan_std_dev\n",
    "        self.resize_shape = resize_shape\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def get_frames(filename):\n",
    "        '''\n",
    "        method for getting the frames from a video file\n",
    "        args: \n",
    "            filename: exact path of the video file\n",
    "            first_only: whether to detect the first frame only or all of the frames\n",
    "        out:\n",
    "            video_frames, label:  \n",
    "        '''\n",
    "\n",
    "        filepath = filename.numpy().decode('utf-8')\n",
    "\n",
    "\n",
    "        cap = cv2.VideoCapture(filepath) \n",
    "        # captures the video. Think of it as if life is a movie so we ask the method to focus on patricular event\n",
    "        # that is our video in this case. It will concentrate on the video\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        all_frames = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "\n",
    "\n",
    "        fc = 0\n",
    "        while(cap.isOpened() and fc < frameCount): # as long as all the frames have been traversed\n",
    "            ret, frame = cap.read()\n",
    "            # capture the frame. Again, if life is a movie, this function acts as camera\n",
    "\n",
    "            if ret==True:\n",
    "                all_frames[fc] = frame\n",
    "                fc += 1\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'): # break in between by pressing the key given\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        # release whatever was held by the method for say, resources and the video itself\n",
    "        return all_frames\n",
    "\n",
    "    # tensorflow functions to pre-process videos\n",
    "    def normalize(video, chan_means, chan_std_dev):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Arguments:\n",
    "            video {tf.Tensor} -- tensorflow reshaped video data\n",
    "            chan_means {array} -- [description]\n",
    "            chan_std_dev {array} -- [description]\n",
    "\n",
    "        Returns:\n",
    "            [tf.Tensor] -- normalized video data\n",
    "        \"\"\"\n",
    "\n",
    "        video /= 255\n",
    "        video -= chan_means\n",
    "        video /= chan_std_dev\n",
    "\n",
    "        return video\n",
    "\n",
    "    def transform_vid(self, filename):\n",
    "        \n",
    "        chan_means = self.chan_means\n",
    "        chan_std_dev = self.chan_std_dev\n",
    "        resize_shape = self.resize_shape\n",
    "        seq_length = self.seq_length\n",
    " \n",
    "        parts = tf.strings.split(filename, '/')\n",
    "        label = parts[-2]\n",
    "        # Don't want to exceed frames, available, using 198 as limit\n",
    "        if seq_length == 298:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = np.random.randint(298 - seq_length)\n",
    "\n",
    "        vid = get_frames(filename)[start:(start+seq_length),:,:,:]\n",
    "        vid = tf.image.resize(vid, size=resize_shape).numpy()\n",
    "        vid = normalize(vid, chan_means, chan_std_dev)\n",
    "\n",
    "        return vid, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DeepFakeTransformer(resize_shape=(224,224))\n",
    "trf_func = transformer.transform_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vid_ds = vid_ds.map(lambda x: tf.py_function(trf_func, [x], [tf.float32, tf.string]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'REAL', shape=(), dtype=string)\n",
      "(298, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "for vid, label in vid_ds.take(1):\n",
    "    print(label)\n",
    "    print(vid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224, 224]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.resize_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dims = (batch_size, sequence_len, *resize_shape, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 224, 224, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
